{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing several methods for creating out of sample predictive intervals\n",
    "\n",
    "Methods:\n",
    "- Quantile of residuals\n",
    "- Quantile regression\n",
    "- Jacknife+\n",
    "- Bayesian regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from abc import ABC, abstractmethod\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pystan as ps\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Optional, NoReturn, Dict, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSimulator:\n",
    "    \"\"\"Class encapsulating process of simulating data from a linear model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__():\n",
    "        pass\n",
    "    \n",
    "    def generate_lm_data(n_points: int = 10_000,\n",
    "                         pct_train: float = 0.5,\n",
    "                         alpha: float = 1.,\n",
    "                         beta: float = 2.,\n",
    "                         sigma: float = np.sqrt(3.)) -> Dict[str, Dict[str, np.ndarray]]:\n",
    "        \"\"\"Method to generate data according to following linear model:\n",
    "            y = alpha + beta * x + N(0, sigma^2)\n",
    "        Once data is generated, split it into training set and test set.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_points: int\n",
    "            Number of points we want to generate\n",
    "        alpha: float, optional, default = 1.\n",
    "            Intercept parameter of the linear model\n",
    "        beta: float, optional, default = 2.\n",
    "            Explanatory variable parameter of the linear model\n",
    "        sigma: float, optional, default = sqrt(3.)\n",
    "            Standard deviation of the normal error\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        data: Dict[str, np.ndarray]\n",
    "            Dictionary containing dictionary traing and test data\n",
    "        \"\"\"\n",
    "        # Sample data/realization\n",
    "        x = np.random.randn(n_points)\n",
    "        y = alpha + beta*x + sigma * np.random.randn(n_points)\n",
    "\n",
    "        # Split data into traing and test set\n",
    "        n_train = int(pct_train * n_points)\n",
    "\n",
    "        # Create training data and test data\n",
    "        y_train = y[:n_train]\n",
    "        x_train = x[:n_train]\n",
    "\n",
    "        y_test = y[n_train:]\n",
    "        x_test = x[n_train:]\n",
    "\n",
    "        # Stack columns together\n",
    "        data_train = {'y': y_train, 'x': x_train}\n",
    "        data_test = {'y': y_test, 'x': x_test}\n",
    "\n",
    "        data = {'train': data_train,\n",
    "                'test': data_test}\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sim = DataSimulator.generate_lm_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of data points to sample\n",
    "n = 10000\n",
    "pct_train = 0.5\n",
    "\n",
    "# Model parameters\n",
    "sigma = 3.\n",
    "mu = 1.\n",
    "beta = 2.\n",
    "\n",
    "# quantile\n",
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement abstract version of a class for predictive intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictiveIntervalModel(ABC):\n",
    "    \"\"\"Abstract class encapsulating methods reletaed to child classes that implement\n",
    "    different ways of getting predictive intervals.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha: float):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        alpha: float, in interval (0,1)\n",
    "            Probability level at which we want the predictive interval to be, i.e. alpha = 0.1 corresponds\n",
    "            to 90% probability interval.\n",
    "            \n",
    "        \"\"\"\n",
    "        self.check_alpha(alpha=alpha)\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_predictive_intervals(self, data: Dict[str, Dict[str, np.ndarray]]) -> np.ndarray:\n",
    "        \"\"\" Method for that encapsulates the creation and evaluation of predictive interval.\n",
    "        Should print out statistics\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data: Dictionary\n",
    "            Data dictionary with keys 'train' and 'test' corresponding to data we want to use for\n",
    "            training the model and for testing, aka creating predicitve intervals for\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        pred_ints: np.ndarray\n",
    "            2-d numpy array with the dimensions being lower and upper bounds of the created predictive intervals\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def check_alpha(self, alpha: float) -> bool:\n",
    "        \"\"\"Method for asserting alpha is a float between 0 and 1\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        alpha: float\n",
    "            Level \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            True if alpha is between 0 and 1\n",
    "            \n",
    "        Raises\n",
    "        ------\n",
    "        Exception if alpha is outside of (0, 1) interval or not float\n",
    "        \"\"\"\n",
    "        \n",
    "        if (isinstance(alpha, float) and (0 < alpha < 1.)):\n",
    "            return True\n",
    "        else:\n",
    "            raise Exception(\"`alpha` must be float between 0. and 1.!\")\n",
    "            \n",
    "    def print_stats(self, hit_ratio: float, avg_length: float) -> NoReturn:\n",
    "        \"\"\"Method for printing statistics for the created predictive intervals.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        hit_ratio: float\n",
    "        avg_length: float\n",
    "        \"\"\"\n",
    "        print(\"The predictive intervals achieved hit ratio of {:.2f}% and have average length of {:.3f}.\".format(100*hit_ratio, avg_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression - residuals quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(PredictiveIntervalModel):\n",
    "    \"\"\"\n",
    "    Class encapsulating methods for related to linear model\n",
    "    \n",
    "        y = alpha + beta_1 * x_1 + ... + beta_p * x_p + eps\n",
    "        \n",
    "    where eps comes from Normal distribution with zero mean and fixed variance sigma.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 params: Optional[np.ndarray] = None, \n",
    "                 alpha: Optional[float] = 0.1):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        alpha: float, in interval (0,1)\n",
    "            Probability level at which we want the predictive interval to be, i.e. alpha = 0.1 corresponds\n",
    "            to 90% probability interval.\n",
    "        params: np.ndarray\n",
    "            Parameters of the linear model we want to use,\n",
    "        \"\"\"\n",
    "        # Init super\n",
    "        super(LinearModel, self).__init__(alpha=alpha)\n",
    "        \n",
    "        # Set parameters if some were provided\n",
    "        self.params = params\n",
    "    \n",
    "    def fit(self, data: Dict[str, np.ndarray]) -> NoReturn:\n",
    "        \"\"\"\n",
    "        Method that fits linear model based on the provided data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data: Dict[str, np.ndarray]\n",
    "            Data dictionary with keys `x` and `y` representing exogenous and engenous data, respectively\n",
    "        \"\"\"\n",
    "        lm = sm.OLS(endog=data['y'], exog=sm.add_constant(data['x']), has_intercept=True)\n",
    "        lm_fit = lm.fit()\n",
    "        self.params = lm_fit.params\n",
    "    \n",
    "    def predict(self, x_new: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Method that predicts new values of the target variable using the provided data and fitted model.\n",
    "        \n",
    "        Paramaters\n",
    "        ----------\n",
    "        x_new: np.ndarray\n",
    "            Array with new data we want to create prediction for\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        y_fit: np.ndarray\n",
    "            Array with predicted values based on x_new\n",
    "        \"\"\"\n",
    "        assert self.params is not None, \"Model has not been fitted yet! Run `.fit` method first.\"      \n",
    "        assert x_new.ndim == len(self.params) - 1, \"Dimension of new data does not correspond to fitted parameters!\"\n",
    "        \n",
    "        # Create y_fit\n",
    "        if x_new.ndim == 1:\n",
    "            y_fit = self.params[0] + self.params[1]*x_new\n",
    "        else:\n",
    "            y_fit = self.params[0] + np.dot(np.expand_dims(self.params[:1], 0), x_new.transpose())\n",
    "        \n",
    "        # Assert y_fit has the same length as the input data\n",
    "        assert len(y_fit) == len(x_new), \"Wrong predict calculation, lengths don't match!\"\n",
    "        \n",
    "        return y_fit\n",
    "    \n",
    "    def predict_with_residuals(self, data: Dict[str, np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Method that predicts new values of the target variable and also calculates respective residuals\n",
    "        based on the actual response variable values.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data: Dict \n",
    "            Dictionary with new regressor data we want to use to create predictions, should contain keys `x` and `y`\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        y_hat, res: Tuple[np.ndarray, np.ndarray]\n",
    "            Fitted values of y and residuals w.r.t. to actual y\n",
    "        \"\"\"\n",
    "        # Assert that `x` and `y` are in the data dict keys\n",
    "        assert 'x' and 'y' in data.keys(), \"Data dictionary must have `x` and `y` keys!\"\n",
    "        \n",
    "        # Assert that x and y have correct lengths\n",
    "        assert len(data['x']) == len(data['y']), \"`x` and `y` data must have the same length!\"\n",
    "        \n",
    "        # Extract the new x and y data\n",
    "        x_new = data['x']\n",
    "        y_new = data['y']\n",
    "        \n",
    "        # Predict the new y\n",
    "        y_hat = self.predict(x_new=x_new)\n",
    "       \n",
    "        # Calculate residuals\n",
    "        res = y_hat - y_new\n",
    "        \n",
    "        return y_hat, res\n",
    "    \n",
    "    def get_predictive_intervals(self, data: Dict[str, Dict[str, np.ndarray]]) -> np.ndarray:\n",
    "        \"\"\"Method that encapsulates process of creating predictive intervals based on train and\n",
    "        test data provided in the data dictionary.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data: Dict\n",
    "            Data dictionary containing train and test data under `train` and `test` keys.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        pred_ints: np.ndarray\n",
    "            2-d numpy array with the dimensions being lower and upper bounds of the created predictive intervals\n",
    "        \"\"\"\n",
    "        # Fit the model\n",
    "        self.fit(data=data['train'])\n",
    "        \n",
    "        # Create new predictions\n",
    "        y_hat, res = self.predict_with_residuals(data=data['test'])\n",
    "        \n",
    "        # Get (1-alpha)-th quantile of absolute values of residuals   \n",
    "        q_alpha = np.quantile(np.abs(res),1-self.alpha)\n",
    "        \n",
    "        # Create lower and upper bound of the predictive interval\n",
    "        y_lb = y_hat - q_alpha\n",
    "        y_ub = y_hat + q_alpha\n",
    "        \n",
    "        # Calculate hit ratio\n",
    "        y_real = data['test']['y']\n",
    "        hit_ratio = np.mean((y_real >= y_lb) & (y_real <= y_ub))\n",
    "        \n",
    "        # Calculate average interval length\n",
    "        avg_length = np.mean(y_ub - y_lb)\n",
    "        \n",
    "        # Print stats\n",
    "        self.print_stats(hit_ratio=hit_ratio, avg_length=avg_length)\n",
    "        \n",
    "        # Stack the lower and upper bound to create predictive intervals\n",
    "        pred_ints = np.vstack([y_lb, y_ub])\n",
    "        \n",
    "        return pred_ints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.fit(data=data_sim['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict new data and get residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_hat, res = lm.predict_with_residuals(data_sim['test']['x'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get predictive intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = lm.get_predictive_intervals(data=data_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates prediction band using in sample residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_list = []\n",
    "\n",
    "for i in range(N_samples):\n",
    "\n",
    "    # Fit regression model on the first half of the data\n",
    "    x1_const = sm.add_constant(x1)\n",
    "    lm = sm.OLS(y1, x1_const, has_intercept=True)\n",
    "    lm_fit = lm.fit()\n",
    "\n",
    "    # Extract alpha quantile of the residuals\n",
    "    q_alpha = np.quantile(np.abs(lm_fit.resid), 1-alpha)\n",
    "\n",
    "    # Extract coefficients from the estimated regression\n",
    "    alpha_fit, beta_fit = tuple(lm_fit.params)\n",
    "\n",
    "    # Estimate y2 using the estimated coefficients from first half of the dataset\n",
    "    y2_hat = alpha_fit + beta_fit*x2 \n",
    "\n",
    "    # Create range for y2\n",
    "    y2_range = (y2_hat - q_alpha, y2_hat + q_alpha)\n",
    "\n",
    "    # Calculate percentage of data points actually in the range\n",
    "    pct_in_range = np.mean((y2 >= y2_range[0]) & (y2 <= y2_range[1]))\n",
    "    \n",
    "    pct_list.append(pct_in_range)\n",
    "    #print(\"There was {:.2f}% of the data points in the range in experiment {}!\".format(100*pct_in_range, i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_total = 100*np.mean(np.array(pct_list) > 0.95)\n",
    "print(\"Percentage of experiments in which at least {:.1f}% of the predictions fell into the coverage interval = {:.1f}%\".format(100*(1-alpha),pct_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantile regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data_sim['train']['x'], data_sim['train']['y'])\n",
    "plt.plot(plt_data[1,:], np.dot(res_5.params.values, plt_data), color='r')\n",
    "plt.plot(plt_data[1,:], np.dot(res_95.params.values, plt_data), color='k')\n",
    "plt.plot(plt_data[1,:], np.dot(res_med.params.values, plt_data), color='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantileRegression(PredictiveIntervalModel):\n",
    "    \"\"\"Class encapsulating estimation of predictive intervals using quantile regression\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 params_lb: Optional[np.ndarray] = None, \n",
    "                 params_ub: Optional[np.ndarray] = None,\n",
    "                 alpha: Optional[float] = 0.1):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        params_lb: Optional np.ndarray or None\n",
    "            Parameters of the lower bound quantile regression, i.e. alpha level\n",
    "        params_ub: Optional np.ndarray or None\n",
    "            Parameters of the upper bound quantile regression, i.e. 1-alpha level\n",
    "        alpha: float in (0,1), optional, default value = 0.1\n",
    "            Quantile we want to estimate. Regression will esimate alpha/2-th and (1-alpha/2)-th quantile\n",
    "            to create predictive interval at 1-alpha level\n",
    "        \"\"\"\n",
    "        super(QuantileRegression, self).__init__(alpha=alpha)\n",
    "        self.params_lb = params_lb\n",
    "        self.params_ub = params_ub\n",
    "    \n",
    "    def fit(self, data: Dict[str, np.ndarray]) -> NoReturn:\n",
    "        \"\"\"Method that fits lower (1-alpha/2) and upper (alpha/2) quantile regression based on the\n",
    "        provided data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data: Dictionary\n",
    "            Data dictionary with keys `x` and `y` corresponding to endogenous and exogenous variables.\n",
    "        \"\"\"\n",
    "        assert 'x' and 'y' in data.keys(), \"Data dictionary must have `x` and `y` keys!\"\n",
    "        \n",
    "        # Specify quantile regression model\n",
    "        mod = smf.quantreg('y ~ x', pd.DataFrame(data))\n",
    "        \n",
    "        # Fit the lower and upper quantile regression\n",
    "        res_lb = mod.fit(q=self.alpha/2)\n",
    "        res_ub = mod.fit(q=(1-self.alpha/2))\n",
    "        \n",
    "        # Save the fitted parameters into self\n",
    "        self.params_lb = res_lb.params.values\n",
    "        self.params_ub = res_ub.params.values\n",
    "        \n",
    "    def predict(self, data: Dict[str, np.ndarray]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Method that predicts \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data: Dictionary\n",
    "            Data dictionary with keys `x` and `y` corresponding to endogenous and exogenous variables.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_lb, y_up: Tuple[np.ndarray, np.ndarray]\n",
    "            Tuple of numpy array's for lower and upper bound predictions for y\n",
    "        \"\"\"\n",
    "        # Assert that `x` and `y` are in the data dict keys\n",
    "        assert 'x' and 'y' in data.keys(), \"Data dictionary must have `x` and `y` keys!\"\n",
    "        \n",
    "        # Assert that x and y have correct lengths\n",
    "        assert len(data['x']) == len(data['y']), \"`x` data and `y` data must be of the same length!\"\n",
    "            \n",
    "        # Extract the new x and y data (have to add intercept to the `x` data)\n",
    "        x_new = np.vstack([np.ones(len(data['x'])), data['x']])\n",
    "\n",
    "        # Predict the lower and upper bound for y\n",
    "        y_lb = np.dot(self.params_lb, x_new)\n",
    "        y_ub = np.dot(self.params_ub, x_new)\n",
    "        \n",
    "        return y_lb, y_ub\n",
    "    \n",
    "    def get_predictive_intervals(self,\n",
    "                                 data: Dict[str, Dict[str, np.ndarray]]) -> np.ndarray:\n",
    "        \"\"\"Method that encapsulates process of creating predictive intervals based on train and\n",
    "        test data provided in the data dictionary.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data: Dict\n",
    "            Data dictionary containing train and test data under `train` and `test` keys.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        pred_ints: np.ndarray\n",
    "            2-d numpy array with the dimensions being lower and upper bounds of the created predictive intervals\n",
    "        \"\"\"\n",
    "        self.fit(data=data['train'])\n",
    "        y_lb, y_ub = self.predict(data['test'])\n",
    "        \n",
    "        # Calculate hit ratio\n",
    "        y_real = data['test']['y']\n",
    "        hit_ratio = np.mean((y_real >= y_lb) & (y_real <= y_ub))\n",
    "        \n",
    "        # Calculate average interval length\n",
    "        avg_length = np.mean(y_ub - y_lb)\n",
    "        \n",
    "        # Print stats\n",
    "        self.print_stats(hit_ratio=hit_ratio, avg_length=avg_length)\n",
    "        \n",
    "        # Stack the lower and upper bound to create predictive intervals\n",
    "        pred_ints = np.vstack([y_lb, y_ub])\n",
    "        \n",
    "        return pred_ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qr = QuantileRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qr.fit(data=data_sim['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = qr.get_predictive_intervals(data=data_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jacknife+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JacknifePlus(PredictiveIntervalModel):\n",
    "    \"\"\"Class encapsulating implementation of the Jacknife+ method\n",
    "    Jacknife+ paper on arxiv: https://arxiv.org/pdf/1905.02928.pdf\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha: Optional[float] = 0.1):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        alpha: float in (0,1), optional, default value = 0.1\n",
    "            Quantile we want to estimate. Regression will esimate alpha/2-th and (1-alpha/2)-th quantile\n",
    "            to create predictive interval at 1-alpha level\n",
    "        \"\"\"\n",
    "        \n",
    "        super(JacknifePlus, self).__init__(alpha=alpha)\n",
    "        \n",
    "        # Attributes for storing residuals and fitted models\n",
    "        self.residuals = None\n",
    "        self.fitted_lms = None\n",
    "    \n",
    "    def fit_linear_model(self, data: Dict[str, np.ndarray]):\n",
    "        \"\"\"\n",
    "        Method that fits linear model based on the provided data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data: Dict[str, np.ndarray]\n",
    "            Data dictionary with keys `x` and `y` corresponding to exogenous and endogenous variable, respectively\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        lm_fit: \n",
    "        \"\"\"\n",
    "        lm = sm.OLS(data['y'], sm.add_constant(data['x']), has_intercept=True)\n",
    "        lm_fit = lm.fit()\n",
    "        return lm_fit\n",
    "          \n",
    "    def fit(self, data: Dict[str, np.ndarray]) -> NoReturn:\n",
    "        \"\"\"\n",
    "        Method that fits linear model on every leave-one-out subset of the data and collects\n",
    "        residuals for the leave-one-out point based on the trained model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data: Dict[str, np.ndarray]\n",
    "        \"\"\"\n",
    "        # Extract training data\n",
    "        y_train = data['y']\n",
    "        x_train = data['x']\n",
    "        \n",
    "        # Get number of data points in the data\n",
    "        n = len(y_train)\n",
    "\n",
    "        # Store residuals and fitted models\n",
    "        res = np.zeros(n)\n",
    "        fitted_lms = []\n",
    "\n",
    "        for i in range(n):\n",
    "            # Single out the point we're not using in this estimation\n",
    "            y_out = y_train[i]\n",
    "            if x_train.ndim == 1:\n",
    "                x_out = x_train[i]\n",
    "            else:\n",
    "                x_out = x_train[i, :]\n",
    "\n",
    "            # Estimate linear regression leaving out the point\n",
    "            data = {'x': np.delete(x_train, i, axis=0),\n",
    "                    'y': np.delete(y_train, i, axis=0)}\n",
    "            lm_fit = self.fit_linear_model(data=data)\n",
    "\n",
    "            # Estimate the point we left out using the fitted linear model\n",
    "            y_hat = lm_fit.predict(exog=np.array((1, x_out)))\n",
    "\n",
    "            # Calculate the residual\n",
    "            res[i] = y_hat - y_out\n",
    "\n",
    "            # Append fitted model\n",
    "            fitted_lms.append(lm_fit)\n",
    "\n",
    "        # Save to self\n",
    "        self.residuals = res\n",
    "        self.fitted_lms = fitted_lms\n",
    "        \n",
    "    def get_predictive_intervals(self, \n",
    "                                 data: Dict[str, Dict[str, np.ndarray]],\n",
    "                                 pct_sample: float = 1.) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        \n",
    "        Paramateres\n",
    "        -----------\n",
    "        \"\"\"\n",
    "        if self.fitted_lms or self.residuals is None:\n",
    "            self.fit(data=data['train'])\n",
    "            \n",
    "        # Add constant term for intercept\n",
    "        x_new = sm.add_constant(data['test']['x'])\n",
    "        \n",
    "        # Get number of points and number of fitted models\n",
    "        n_points = int(pct_sample*len(x_new))\n",
    "        n_models = len(self.fitted_lms)\n",
    "        \n",
    "        # Array for predictive intervals\n",
    "        y_lb = np.zeros(n_points)\n",
    "        y_ub = np.zeros(n_points)\n",
    "        \n",
    "        # Idx sample to subset data\n",
    "        idx_data = np.random.choice(len(x_new), size=n_points,replace=False)\n",
    "        \n",
    "        # Loop over all points in the new data\n",
    "        for i in range(n_points):\n",
    "            \n",
    "            # Dictionary with upper and lower bounds created by k-th model\n",
    "            y_jacknife = {'lb': np.zeros_like(self.residuals),\n",
    "                          'ub': np.zeros_like(self.residuals)}\n",
    "            \n",
    "            # Loop over all fitted models\n",
    "            for k in range(n_models):\n",
    "                \n",
    "                # Get the jacknife+ point estimate using the k-th fitted model\n",
    "                y_hat_k = self.fitted_lms[k].predict(exog=x_new[idx_data[i],:])\n",
    "                \n",
    "                # Construct the jacknife+ intervals\n",
    "                y_jacknife['lb'][k] = y_hat_k - self.residuals[k]\n",
    "                y_jacknife['ub'][k] = y_hat_k + self.residuals[k]\n",
    "               \n",
    "            # Calculate the quantile of the jacknife+ quantities \n",
    "            y_lb[i] = np.quantile(y_jacknife['lb'], self.alpha/2)\n",
    "            y_ub[i] = np.quantile(y_jacknife['ub'], 1-self.alpha/2)\n",
    "\n",
    "        # Calculate hit ratio\n",
    "        y_real = data['test']['y'][idx_data]\n",
    "        hit_ratio = np.mean((y_real >= y_lb) & (y_real <= y_ub))\n",
    "        \n",
    "        # Calculate average interval length\n",
    "        avg_length = np.mean(y_ub - y_lb)\n",
    "        \n",
    "        # Print stats\n",
    "        self.print_stats(hit_ratio=hit_ratio, avg_length=avg_length)\n",
    "        \n",
    "        # Stack the lower and upper bound to create predictive intervals\n",
    "        pred_ints = np.vstack([y_lb, y_ub])\n",
    "        \n",
    "        return pred_ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jknf = JacknifePlus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = jknf.get_predictive_intervals(data=data_sim, pct_sample=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianRegression(PredictiveIntervalModel):\n",
    "    \"\"\"Bayesian regression implemented in PyStan\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha: Optional[float]=0.1):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        alpha: float\n",
    "        \"\"\"\n",
    "        super(BayesianRegression, self).__init__(alpha=alpha)\n",
    "        \n",
    "    def get_stan_model(self) -> str:\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        model: str\n",
    "            String specification of Stan model\n",
    "        \"\"\"\n",
    "        \n",
    "        model = \"\"\"\n",
    "        data {\n",
    "            int<lower=0> N;\n",
    "            int<lower=0> N_test;\n",
    "            vector[N] x_train;\n",
    "            vector[N] y_train;\n",
    "            vector[N] x_test;\n",
    "        }\n",
    "        parameters {\n",
    "            real alpha;\n",
    "            real beta;\n",
    "            real<lower=0> sigma;\n",
    "            vector[N_test] y_hat;\n",
    "        }\n",
    "        model {\n",
    "            y_train ~ normal(alpha + beta * x_train, sigma);\n",
    "            y_hat ~ normal(alpha + beta * x_test, sigma);\n",
    "        }\n",
    "\n",
    "        \"\"\"\n",
    "        return model\n",
    "\n",
    "    def get_predictive_intervals(self, data: Dict[str, Dict[str, np.ndarray]]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        Implementation inspired by the following article:\n",
    "        https://towardsdatascience.com/an-introduction-to-bayesian-inference-in-pystan-c27078e58d53\n",
    "        \n",
    "        and the following pystan tutorial:\n",
    "        https://mc-stan.org/docs/2_21/stan-users-guide/prediction-forecasting-and-backcasting.html\n",
    "        \"\"\"\n",
    "        # Specify stan model\n",
    "        model = self.get_stan_model()\n",
    "        \n",
    "        # Put model data into dictionary\n",
    "        data_stan = {'N': len(data['train']['x']),\n",
    "                     'x_train': data['train']['x'],\n",
    "                     'y_train': data['train']['y'],\n",
    "                     'N_test': len(data['test']['x']),\n",
    "                     'x_test': data['test']['x']}\n",
    "\n",
    "        # Compile pystan model\n",
    "        sm = ps.StanModel(model_code=model)\n",
    "\n",
    "        # Fit the model and generate fitted predictions\n",
    "        fit = sm.sampling(data=data_stan, iter=1000, chains=4, warmup=500, thin=1, seed=101).to_dataframe()\n",
    "        \n",
    "        # Extract real y, prepare arrays for lower and upper bound\n",
    "        y_real = data['test']['y']\n",
    "        y_lb = np.zeros_like(y_real)\n",
    "        y_ub = np.zeros_like(y_real)\n",
    "        \n",
    "        # Loop over the generated samples and extract their distributional property\n",
    "        for i in range(data_stan['N_test']):\n",
    "            y_hat_name = \"y_hat[{}]\".format(i+1)\n",
    "            y_lb[i] = np.quantile(fit[y_hat_name], self.alpha/2)\n",
    "            y_ub[i] = np.quantile(fit[y_hat_name], 1-self.alpha/2)\n",
    "        \n",
    "        # Calculate hit ratio\n",
    "        hit_ratio = np.mean((y_real >= y_lb) & (y_real <= y_ub))\n",
    "        \n",
    "        # Calculate average interval length\n",
    "        avg_length = np.mean(y_ub - y_lb)\n",
    "        \n",
    "        # Print stats\n",
    "        self.print_stats(hit_ratio=hit_ratio, avg_length=avg_length)\n",
    "        \n",
    "        # Stack the lower and upper bound to create predictive intervals\n",
    "        pred_ints = np.vstack([y_lb, y_ub])\n",
    "        \n",
    "        return pred_ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "br = BayesianRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ints = br.get_predictive_intervals(data_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_experiments = 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
